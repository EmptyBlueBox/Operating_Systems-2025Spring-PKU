## 0. 核心内容大纲

- [ ] 文件和文件目录
- [ ] 文件系统的实现
- [ ] 文件系统实例
- [ ] Windows NTFS简介
- [x] 文件系统的管理
- [x] 文件系统的性能
- [x] 文件系统结构

---

## 1.文件系统的管理

文件系统管理涉及确保文件系统的一致性、安全性等方面。

### 1.1 文件系统的可靠性

* **可靠性 (Reliability) 定义**: 指文件系统抵御和预防各种物理性破坏 (如磁盘损坏) 和人为性破坏 (如误删除、恶意攻击) 的能力。

* **面临的问题与对策**:
    * **坏块问题 (Bad Blocks Problem)**: 磁盘上可能出现物理损坏的扇区或块，导致数据无法读写。操作系统需要机制来检测、标记并绕过这些坏块。
    * **备份 (Backup)**: 通过转储操作，创建文件或整个文件系统的多个副本，以便在发生数据丢失或损坏时进行恢复。

* **文件系统备份的类型**:
    * **全量转储 (Full Dump/Backup)**:
        * **解释**: 定期将所有文件完整地拷贝到后援存储器 (如磁带、另一块硬盘、云存储) 。
        * **优点**: 恢复时操作相对简单，只需恢复最近一次的全量备份。
        * **缺点**: 备份时间长，占用存储空间大。
    * **增量转储 (Incremental Dump/Backup)**:
        * **解释**: 只转储自上次备份 (全量或增量) 以来被修改过的文件。
        * **优点**: 备份速度快，占用存储空间小，减少系统开销。
        * **缺点**: 恢复过程可能较复杂，需要最近一次全量备份以及之后所有的增量备份。
    * **物理转储 (Physical Dump)**:
        * **解释**: 从磁盘的第0块开始，按顺序将所有磁盘块的内容原样输出到备份介质 (如磁带) 。不关心文件系统结构，只复制原始数据。
        * **优点**: 简单直接，可以备份整个磁盘，包括引导扇区和未被文件系统使用的部分。
        * **缺点**: 可能备份大量无用数据 (如空闲块) ，恢复时通常也需要恢复整个磁盘。
    * **逻辑转储 (Logical Dump)**:
        * **解释**: 从一个或几个指定的目录开始，递归地转储自给定日期 (或上次备份) 之后所有被更改过的文件和目录。它基于文件系统的逻辑结构 (目录和文件) 。
        * **优点**: 备份内容更具针对性，可以只备份重要的用户数据，通常不备份空闲空间。
        * **缺点**: 备份和恢复速度可能受文件数量和目录结构复杂度的影响。

### 1.2 文件系统一致性 (File System Consistency)

* **名词解释**: 文件系统一致性指的是文件系统的元数据 (如目录结构、i-node表、空闲块列表等) 和实际存储的数据之间保持逻辑上的正确和同步状态。

* **问题产生的根源**:
    * 操作系统通常会将磁盘块的内容读入内存进行修改 (例如，更新文件内容、修改目录项、分配/释放磁盘块) 。
    * 修改后的数据块会暂时缓存在内存中，等待合适的时机写回磁盘。
    * 如果在数据成功写回磁盘之前，系统发生崩溃 (如断电、内核错误) ，内存中的修改就会丢失，导致磁盘上的文件系统状态与预期不符，出现不一致。

* **解决方案**:
    * 设计一个实用程序 (如 Unix/Linux 下的 `fsck` - file system check) 。
    * 该程序在系统再次启动时运行，负责检查磁盘块和目录系统，发现并尝试修复不一致性。
    * **`fsck` (File System Check and Repair)**: 是一个常用的命令行工具，用于检查并可选地修复Unix和类Unix操作系统中的文件系统错误。

* **例子：UNIX 一致性检查工作过程**:
    * **核心思想**: 通过对比元数据中记录的磁盘块使用情况和空闲块列表，来发现不一致。
    * **实现机制**:
        1.  **两张表 (Counters for Blocks)**: 系统维护两张表 (或等效的数据结构) ，每个磁盘块在每张表中都有一个对应的计数器，初始值都为0。
            * **表1 (Blocks in Files Counter)**: 记录每个磁盘块在各个文件中出现的次数。理想情况下，一个已分配的磁盘块应该只出现在一个文件中一次。
            * **表2 (Blocks in Free List Counter)**: 记录每个磁盘块在空闲块列表 (或位图) 中出现的次数。理想情况下，一个空闲块应该在空闲列表中出现一次。
        2.  **扫描与计数**: `fsck` 程序会扫描整个文件系统：
            * 遍历所有文件及其包含的磁盘块，对表1中对应块的计数器加1。
            * 遍历空闲块列表，对表2中对应块的计数器加1。
        3.  **一致性检查规则 (参照示意图)**:
            * **情况1 (Block in Use and Free - 不一致)**: 如果一个块的计数器在表1中大于0 (即在文件中使用) ，同时在表2中也大于0 (即在空闲列表中) ，则文件系统不一致。**处理**: 通常会选择从空闲列表中移除该块。
            * **情况2 (Block Used Multiple Times in Files - 不一致)**: 如果一个块在表1中的计数器大于1 (即被多个文件或一个文件的多个地方引用) ，则文件系统不一致。**处理**: 这通常是严重错误，可能需要清除对该块的引用，或者复制该块并让其中一个文件引用副本。
            * **情况3 (Block in Free List Multiple Times - 不一致)**: 如果一个块在表2中的计数器大于1 (即在空闲列表中重复出现) ，则文件系统不一致。**处理**: 从空闲列表中移除重复的条目。
            * **情况4 (Block in Use but Not in Free List - 一致)**: 表1计数器 > 0, 表2计数器 = 0。这是已分配块的正常状态。
            * **情况5 (Block Free but Not in Use - 一致)**: 表1计数器 = 0, 表2计数器 > 0。这是空闲块的正常状态。
            * **情况6 (Block Not in Use and Not in Free List - 丢失块)**: 表1计数器 = 0, 表2计数器 = 0。这表示块既没被文件使用，也不在空闲列表中，是“丢失块”。**处理**: 通常会将其添加到空闲列表中。

* **文件系统写入方式 (File System Write Methods - 影响一致性和性能)**:
    * **(1) 通写 (Write-through)**:
        * **解释**: 内存中的数据修改会立即、同步地写回到磁盘上。只有当数据成功写入磁盘后，写操作才算完成。
        * **优点**: 可靠性高，即使系统崩溃，数据丢失的风险也较小，因为最近的修改已经落盘。
        * **缺点**: 性能差，因为每次写操作都要等待慢速的磁盘I/O。
        * **例子**: FAT文件系统 (如MS-DOS、早期Windows的U盘格式) 常采用这种方式或类似方式保证简单性和一定的可靠性。
    * **(2) 延迟写 (Lazy-write / Write-back)**:
        * **解释**: 修改首先在内存中的缓存 (Write-back cache) 进行。数据不会立即写回磁盘，而是等待一段时间 (如缓存满、固定时间间隔、文件关闭) 后批量写回。
        * **优点**: 性能好，写操作可以快速完成 (因为只写内存) ，多个小写操作可以合并成一个大的磁盘写操作。
        * **缺点**: 可恢复性差。如果在数据写回磁盘前系统崩溃，内存中的修改会丢失，导致文件系统不一致。
    * **(3) 可恢复写 (Transaction Log / Journaling)**:
        * **解释**: 采用事务日志 (Journaling) 来实现文件系统的写入。在对文件系统元数据 (有时也包括数据) 进行实际修改之前，先将要做的修改操作 (或修改本身) 以日志条目的形式写入一个专门的日志区域。只有日志写入成功后，才进行实际修改。如果系统崩溃，重启后可以通过回放日志来完成未完成的操作或撤销部分完成的操作，从而恢复一致性。
        * **优点**: 较好地平衡了安全性和性能。比通写快，比延迟写安全。
        * **例子**: NTFS (Windows), ext3/ext4 (Linux), APFS (macOS)。

### 1.3 文件系统的安全性

* **安全性 (Security) 定义**: 确保文件系统中的数据不会被未经授权的用户访问、修改或破坏。

* **数据丢失 (Data Loss) 的原因及对策**:
    * **灾难 (Disasters)**: 如火灾、水灾、地震等物理损坏。
    * **硬件或软件故障 (Hardware/Software Failures)**: 如磁盘损坏、控制器故障、操作系统bug。
    * **人的失误 (Human Errors)**: 如误删除、误操作。
    * **解决方案**: 主要通过**备份 (Backup)** 机制来恢复。

* **入侵者 (Intruders) 的类型**:
    * **消极的 (Passive Intruders)**: 只是窃听或窥视数据，不修改数据。
        * **非技术人员的偶然窥视 (Casual Eavesdropping by Non-technical Personnel)**: 如无意中看到他人屏幕。
        * **入侵者的窥视 (Deliberate Snooping by Intruders)**: 有意获取未授权信息。
    * **积极的 (Active Intruders)**: 会修改、删除数据或破坏系统。
        * **明确的偷窃企图 (Clear Theft Attempts)**: 试图窃取敏感数据。
        * **商业或军事间谍活动 (Commercial or Military Espionage)**: 有组织的、高技术的攻击。
    * **安全设计考量**: 在设计安全机制时，需要考虑要防范的是哪一类入侵者，因为不同类型的入侵者能力和动机不同，需要的防护级别也不同。

* **文件的保护机制 (File Protection Mechanisms)**:

    * **(1) 文件保护 (File Protection) 概述**:
        * **定义**: 用于提供安全性的、特定的操作系统机制。
        * **目标**:
            * 对拥有权限的用户，应该让其进行相应的操作 (如读、写、执行) 。
            * 对没有权限的用户，应禁止其操作。
            * 防止其他用户冒充合法用户对文件进行操作。
        * **实现手段**:
            * **用户身份验证 (User Authentication)**: 确认用户的身份。
            * **访问控制 (Access Control)**: 根据用户身份和权限，决定其对文件的访问能力。
        * **核心原则**: 文件数据不能被随意访问。

    * **(2) 用户身份验证 (User Authentication)**:
        * **时机**: 当用户登录系统或请求访问受保护资源时，检验其身份。
        * **依据 (三要素)**: 用户是谁 (身份) ，用户拥有什么 (物理凭证) ，用户知道什么 (秘密信息) 。
        * **常见方法**:
            * **口令、密码 (Passwords, Passphrases)**: 用户知道的秘密。
            * **物理鉴定 (Physical Identification)**:
                * 磁卡 (Magnetic Stripe Cards)
                * 签名分析 (Signature Analysis)
            * **基于生物特征信息的认证 (Biometric Authentication)**:
                * 指纹 (Fingerprint)
                * 虹膜 (Iris Scan)
                * 视网膜 (Retina Scan)
                * 人脸识别 (Facial Recognition)
                * 声纹 (Voiceprint)
                * 手掌血管 (Palm Vein)
                * 红外线人脸 (Infrared Facial Recognition)
                * 步态 (Gait Recognition)
                * 笔迹 (Handwriting Analysis)
            * **CAPTCHA 测试 (Completely Automated Public Turing test to tell Computers and Humans Apart)**: 用于区分人类用户和计算机程序。

    * **(3) 访问控制 (Access Control)**:
        * **思考这两种不同访问控制方式的优缺点**:
            * **访问控制表 (ACL) 的优缺点**:
                * **优点**: 以资源为中心，直观易懂，很容易查看某个文件有哪些用户可以访问以及具体权限。细粒度控制较好。
                * **缺点**: 当用户数量和文件数量巨大时，ACL可能非常庞大，管理和存储开销大。权限变更 (如用户离职，权限回收) 可能需要修改多个ACL。检查权限时可能需要遍历较长的列表。
            * **能力表 (Capability List) 的优缺点**:
                * **优点**: 以用户为中心，用户持有的能力直接表明其可访问的资源。权限检查通常较快 (验证凭证) 。权限传递相对容易 (但需小心控制) 。
                * **缺点**: 难以确定某个特定文件有哪些用户可以访问 (需要检查所有用户能力表) 。能力凭证的撤销和管理可能比较复杂 (一旦发出，如何使其失效) 。能力凭证可能被窃取或滥用。

        * **主动控制：访问控制表 (Access Control List - ACL)**:
            * **核心思想**: “谁能访问我？” (Resource-centric)
            * **结构**:
                * 每个文件 (或资源对象) 拥有一个ACL。
                * 通常存放在内核空间，由操作系统管理。
                * ACL中记录了用户 (或主体，如进程、组) 的标识 (User ID) 以及对应的访问权限 (如读、写、执行) 。
                * 用户可以是一个用户组。
                *  (广义上) 文件也可以是一组文件 (如目录的ACL影响其下文件) 。
            * **工作方式**: 当用户尝试访问文件时，操作系统会检查该文件上的ACL，看是否有匹配当前用户的条目，并根据条目中的权限决定是否允许操作。
            * **常见场景：Unix 9-bit 模式**:
                * **解释**: Unix/Linux系统中一种简化的ACL实现。
                * **权限分组**: 分为三组权限：
                    * **所有者 (Owner)**: 文件创建者或指定的所有者。
                    * **所属组 (Group)**: 文件所属的用户组。
                    * **其他用户 (Others)**: 系统中既非所有者也非所属组成员的其他所有用户。
                * **权限位**: 每一组都有三位权限：
                    * **读 (Read - r)**: 查看文件内容。
                    * **写 (Write - w)**: 修改文件内容。
                    * **执行 (Execute - x)**: 如果是程序文件则运行它，如果是目录则进入它。
                * **匹配与检查**: 系统通过用户ID (UID) 和组ID (GID) 来匹配文件权限的相应部分。
                * **命令示例**: `chmod +x filename` (给文件增加执行权限), `chmod 777 filename` (给所有者、组、其他用户都赋予读、写、执行权限，7代表rwx: 4(r)+2(w)+1(x)=7)。

        * **能力表 (Capabilities / Capability Lists)**:
            * **核心思想**: “我能访问谁？” (User/Subject-centric)
            * **结构**:
                * 每个用户 (或主体) 持有一个能力表。
                * 通常存放在内核空间或由可信服务管理。
                * 能力表中记录了该用户可以访问的文件名 (或资源标识符) 以及相应的访问权限。
                * 用户可以是一个用户组 (通过角色或组能力) 。
                * 文件可以是一组文件 (通过通配符或目录能力) 。
            * **工作方式**: 用户在访问资源时，需要向系统出示其拥有的相应能力凭证。系统校验该凭证的有效性和权限。
            * **常见运用场景**:
                * **Kerberos**: 一种网络认证协议，用户获取票据 (ticket，一种能力凭证) 来访问服务。
                * **OAuth**: 一种开放授权标准，允许第三方应用访问用户在某服务提供商上的资源，而无需获取用户密码，通过令牌 (token，一种能力凭证) 实现。

        * **拓展：RBAC 与 ABAC (Role-based and Attribute-based Access Control)**:
            * **RBAC (Role-based Access Control - 基于角色的访问控制)**:
                * **解释**: 用户不直接被授予权限，而是被分配一个或多个角色 (如管理员、编辑、访客) 。每个角色被授予一组特定的权限。
                * **优点**: 当用户职责变化时，只需修改用户被分配的角色，而无需逐个修改其具体权限配置，简化了权限管理，尤其在组织结构复杂的环境中。
            * **ABAC (Attribute-based Access Control - 基于属性的访问控制)**:
                * **解释**: 访问决策基于分配给主体、客体、操作和环境条件的属性。例如，用户属性 (如部门、职位) 、环境属性 (如时间、地点、设备安全状态) 和资源属性 (如数据敏感级别) 。
                * **优点**: 允许制定非常细粒度和动态的访问控制策略，能更好地适应复杂和变化的业务规则。适合批量调整访问控制权限 (对比ACL需要逐个调整用户权限，或能力表需要回收凭证) 。
                * **应用场景**: 公司雇员权限管理系统 (如根据员工级别、项目归属、当前任务等动态授予不同系统模块的访问权限) 。

    * **(4) UNIX 的文件保护 (UNIX File Protection - 结合了用户分类和操作权限)**:
        * **核心**: 审查用户的权限，审查本次操作的合法性。
        * **机制**: 采用文件的二级存取控制。
            * **第一级：对访问者的识别 (User Categorization)**:
                * **文件主 (Owner)**: 通常是文件的创建者。
                * **文件主的同组用户 (Group)**: 与文件主属于同一个用户组的用户。
                * **其它用户 (Others)**: 系统中不属于以上两类的所有其他用户。
            * **第二级：对操作权限的识别 (Operation Permissions)**:
                * **读操作 (r - read)**
                * **写操作 (w - write)**
                * **执行操作 (x - execute)**
                * **不能执行任何操作 (- - no permission)**
        * **例子**: `rwx rwx rwx` (三组，每组对应Owner, Group, Others的r,w,x权限)
            * `chmod 711 file1`:
                * Owner: `7` (二进制 `111`) -> `rwx` (读、写、执行)
                * Group: `1` (二进制 `001`) -> `--x` (仅执行)
                * Others: `1` (二进制 `001`) -> `--x` (仅执行)
            * `chmod 755 file2`:
                * Owner: `7` (二进制 `111`) -> `rwx` (读、写、执行)
                * Group: `5` (二进制 `101`) -> `r-x` (读、执行)
                * Others: `5` (二进制 `101`) -> `r-x` (读、执行)

* **数据恢复技术 (Data Recovery Technology)**:
    * **原理**: 当磁盘、分区、文件遭到破坏 (如删除、格式化) 时，其原始数据内容通常并未立即从物理存储介质上被擦除或覆盖。只是文件系统用于组织和定位这些数据的元数据 (如目录项、文件分配表条目) 被修改或清除了，使得操作系统或用户无法通过常规方式访问到这些数据。数据恢复技术就是尝试重建这些链接或直接从原始数据区恢复信息。
    * **哪些情况下数据不能被恢复？**:
        * **数据被物理覆盖**: 如果原始数据所在的磁盘空间已经被新的数据写入，那么恢复原始数据的可能性极小。
        * **物理损坏严重**: 如磁盘盘片划伤、磁头损坏等，可能导致数据无法读取。
        * **安全擦除**: 使用了专门的数据擦除工具，多次重写数据区域。
        * **加密且密钥丢失**: 如果数据被加密，且解密密钥无法获得。
    * **恢复类型**:
        * **系统数据恢复**: 恢复操作系统文件、分区表等。
        * **用户数据恢复**: 恢复用户的文档、图片、视频等。
    * **恢复手段**:
        * **工具 (Tools)**: 使用专业的数据恢复软件。
        * **手工 (Manual)**: 由专业数据恢复工程师在特定环境下 (如洁净室) 进行物理修复或精细分析。

* **计算机取证技术 (Computer Forensics Technology)**:
    * **名词解释**: 应用计算机科学和调查技术，以合法的方式收集、分析和呈现数字设备 (计算机、手机、存储介质等) 中的电子证据，用于法律诉讼或内部调查。文件系统是计算机取证分析的关键对象之一。

---

## 2. 文件系统的性能

文件系统的性能直接影响用户体验和系统整体效率。磁盘服务的速度和可靠性是系统性能和可靠性的主要瓶颈。

* **核心目标**: 设计文件系统时应尽可能减少磁盘访问次数。

* **提高文件系统性能的方法**:
    * 目录项(FCB)分解 (FCB Decomposition)
    * 当前目录 (Current Directory Caching)
    * 磁盘碎片整理 (Disk Defragmentation)
    * 磁盘(块)高速缓存 (Disk (Block) Caching)
    * 磁盘调度 (Disk Scheduling)
    * 提前读取 (Read-ahead / Prefetching)
    * 合理分配磁盘空间 (Intelligent Disk Space Allocation)
    * 信息的优化分布 (Optimized Information Layout on Disk)
    * RAID技术 (Redundant Arrays of Independent Disks)

### 2.1 磁盘高速缓存 (Disk Cache / Block Cache)

* **定义**: 在内存中为磁盘块设置的一个缓冲区，它保存了磁盘中某些块的副本。
* **工作原理**:
    1.  当应用程序或操作系统需要读取某个磁盘块时，首先检查该块是否存在于磁盘高速缓存中。
    2.  **缓存命中 (Cache Hit)**: 如果块在缓存中，则可以直接从内存中读取，避免了慢速的磁盘I/O。
    3.  **缓存未命中 (Cache Miss)**: 如果块不在缓存中，系统需要从磁盘读取该块到内存中的某个用户缓冲区，并且通常会同时将该块的一个副本存入磁盘高速缓存中。之后再从高速缓存 (或直接从用户缓冲区) 将数据提供给请求者。
* **依据**: **访问的局部性原理 (Principle of Locality of Reference)**。当一个数据块被读入磁盘高速缓存以满足一个I/O请求时，很有可能在不久的将来还会再次访问到这个数据块 (时间局部性) 或其邻近的数据块 (空间局部性) 。
* **别名**: 文件缓存 (File Cache), 块高速缓存 (Block Cache), 缓冲区高速缓存 (Buffer Cache)。

* **相关问题**:
    * **块高速缓存的组织 (Organization)**: 如何组织缓存中的块以便快速查找？ (例如，哈希表) 。
    * **块高速缓存的置换 (Replacement Algorithm)**: 当缓存已满且需要装入新的块时，选择哪个块被替换出去？
        * **例子**: LRU (Least Recently Used - 最近最少使用)。
        * **考量因素**:
            * **该块是否不久后会再次使用?** (LRU等算法试图预测这一点)
            * **该块是否为脏数据 (Dirty Block)?** (即在内存中被修改过但尚未写回磁盘)。脏块在被替换前通常需要先写回磁盘。
            * **该块是否会影响文件系统的一致性?** (如元数据块，可能需要更谨慎的处理策略)。
    * **如何考虑文件系统一致性？**: 与前面提到的写入方式 (通写、延迟写、日志) 紧密相关。缓存中的数据何时写回磁盘以保证数据不丢失和文件系统的一致性。

* **UNIX操作系统 (以XV6为例：文件缓冲)**:
    * **相关头文件**: `#include <kernel/buf.h>`, `#include <kernel/bio.c>`
    * **`bcache`**: 一个全局的块缓冲缓存区。
        * 通常带有一个全局锁，用于同步对缓存的访问。
        * 内部包含固定数量 (如 `NBUF` 个) 的缓存块 (`buf`)。
    * **缓冲块结构 `struct buf`**:
        * `valid`: 标记缓存块中的数据是否有效 (即是否已从磁盘加载) 。
        * `disk`: 标记该缓存块是否正在进行磁盘I/O。
        * `dev`, `blockno`: 记录该缓存块对应于哪个设备上的哪个磁盘块号。
        * `refcnt` (Reference Count): 引用计数，用于判断该缓存块是否仍被某个进程或内核部分使用。只有当 `refcnt` 为0时，该块才可能被回收或替换。
        * `prev`, `next`: 用于将缓存块组织成一个链表，通常是LRU (最近最少使用) 链表，方便实现替换算法。
        * `data`: 实际存储从磁盘读取的块数据的内存区域。

### 2.2 提前读取 (Read-ahead / Prefetching)

* **思路**: 当系统检测到对文件进行顺序读取的模式时，它会预测后续的读取请求，并在实际请求到达之前，就预先将后续的磁盘块读入缓存。
* **依据**: **程序执行的空间局部性原理 (Principle of Spatial Locality)**。如果一个程序访问了某个数据块，那么它很可能接下来会访问其地址上相邻的数据块。
* **开销**: 相对较小。主要的额外开销是数据传输时间，因为寻道和旋转延迟可能已经被前一个实际的读操作分摊了。
* **特点**: 具有针对性，主要对顺序文件访问有效。

### 2.3 Windows 的文件访问方式

* **不使用文件缓冲**:
    * **普通的方式**: 应用程序直接进行文件I/O，操作系统可能不进行额外的缓冲。
    * **`FlushFileBuffers` 函数**: Windows API 函数，用于强制将指定文件的所有缓冲数据写入磁盘。
* **使用文件缓冲 (由 Cache Manager 管理)**:
    * **预读取 (Prefetching)**: 自动进行。系统会根据访问模式决定每次预读取的块大小、使用的缓冲区大小以及缓存满时的置换方式 (通常是LRU的变种)。
    * **写回 (Write-back)**:
        * **写回时机选择**: 例如，数据在缓存中驻留一定时间后、缓存空间不足时、文件关闭时、系统空闲时，或通过特定API调用 (如 `FlushFileBuffers`) 。
        * **一致性问题**: 采用Write-back机制，如果未及时写回，系统崩溃可能导致数据丢失。Windows通过定期 (如默认每秒) 的 Lazy Writer 线程将脏数据写回磁盘，以及在关键操作 (如安全移除硬件) 时强制刷新来维护一致性。
* **异步模式 (Asynchronous Mode)**:
    * 应用程序发起I/O操作后，可以不必等待磁盘操作完成，而是继续执行其他任务。
    * 当I/O操作完成时，系统通过回调函数、事件或其他机制通知应用程序。
    * **优点**: 提高系统吞吐量和响应性，使处理器和I/O设备可以并发工作。

* **Windows 文件访问与 Cache Manager**:
    * 用户对磁盘的访问通常通过访问系统维护的文件缓存来实现。
    * **Cache Manager**: Windows 内核组件，负责管理文件系统缓存。
        * **读取数据时预取 (Prefetch)**。
        * **缓存满时**: 根据LRU原则 (或其变体) 清除缓存内容。
        * **定期更新磁盘**: 定期 (例如每隔1秒) 将缓存中修改过的内容 (脏页) 更新到磁盘上，以保证与Cache的一致性 (Lazy Writer线程) 。
        * **Write-back 机制**: 用户对磁盘写数据时，通常只更改Cache中的内容。Cache Manager决定何时将更新反映到磁盘。
    * **数据拷贝过程 (示意图解释)**:
        * 图中阴影部分表示需要访问的数据。
        * 数据在**磁盘**、**系统Cache空间**和**进程 (用户) 空间**可能存在3份拷贝。
            1.  磁盘上的原始数据。
            2.  系统Cache中的副本。
            3.  应用程序读入其进程地址空间的副本。
        * 一般情况下，用户对数据的修改首先反映在进程空间的副本，然后写到系统Cache中。这些修改并**不直接反映到磁盘**上，而是通过 **Write-back 机制**由 **Lazy Writer** 线程定期地更新到磁盘。

### 2.4 合理分配磁盘空间 (Intelligent Disk Space Allocation)

* **目标**: 在为文件分配磁盘块时，尽量将逻辑上连续 (或可能被顺序访问) 的块存放在物理上也连续或接近的位置。
* **具体做法**: 尽量将属于同一个文件的块，或者预计会被一起访问的多个文件的块，分配在**同一柱面 (Cylinder)** 上。
* **好处**: 减少磁盘臂的移动次数和移动距离，从而显著降低寻道时间，提高访问速度。
* **例子**: **柱面组 (Cylinder Group)** - 一些文件系统 (如UFS) 会将磁盘划分为若干个柱面组，每个柱面组包含若干连续的柱面。文件系统会尝试将一个文件的所有数据块以及其元数据 (如i-node) 都分配在同一个柱面组内，以提高局部性。

### 2.5 磁盘调度 (Disk Scheduling)

* **定义**: 当有多个磁盘访问请求 (读/写) 在队列中等待服务时，操作系统采用一定的策略来调整这些请求的服务顺序。
* **目标**:
    * **降低平均磁盘服务时间**: 通过优化磁头移动路径。
    * **达到公平、高效**:
        * **公平 (Fairness)**: 确保每个I/O请求在有限的时间内得到满足，避免某些请求饿死。
        * **高效 (Efficiency)**: 尽可能减少设备机械运动 (主要是磁头寻道和磁盘旋转) 所带来的时间浪费。
* **一次访盘时间 (Disk Access Time) 的组成**:
    * **寻道时间 (Seek Time)**: 磁头臂从当前柱面移动到目标柱面所需的时间。这是最主要的时间开销。
    * **旋转时间/延迟时间 (Rotational Latency)**: 等待目标扇区旋转到磁头下方所需的时间。
    * **传输时间 (Transfer Time)**: 数据从磁盘扇区读出或写入所需的时间。
* **磁盘调度的主要优化目标**:
    * **减少寻道时间** (主要目标)。
    * **减少延迟时间** (次要，但旋转调度会考虑)。

* **磁盘调度算法 (Disk Scheduling Algorithms)**:
    * **示例场景**:
        * 磁盘访问序列 (道号/柱面号): 98, 183, 37, 122, 14, 124, 65, 67
        * 读写头起始位置: 53
        * **要求计算**: (1) 磁头服务序列；(2) 磁头移动总距离 (道数) 。

    * **(1) 先来先服务 (FCFS - First-Come, First-Served)**:
        * **算法**: 按访问请求到达的先后次序服务。
        * **服务序列**: 53 -> 98 -> 183 -> 37 -> 122 -> 14 -> 124 -> 65 -> 67
        * **磁头移动距离**: (98-53) + (183-98) + (183-37) + (122-37) + (122-14) + (124-14) + (124-65) + (67-65)
            = 45 + 85 + 146 + 85 + 108 + 110 + 59 + 2 = **640 道**
        * **平均移动距离**: 640 / 8 = 80 道
        * **优点**: 简单，公平 (每个请求都会被服务，不会饿死) 。
        * **缺点**: 效率不高。相临两次请求可能会导致磁头在磁盘两端大幅度移动 (例如，从内圈到外圈再回到内圈) ，增加了服务时间，对机械臂也不利。

    * **(2) 最短寻道时间优先 (SSTF - Shortest Seek Time First)**:
        * **算法**: 优先选择距离当前磁头位置最近的访问请求进行服务。即，选择使寻道时间最短的那个请求。
        * **服务序列**:
            1.  当前 53: 最近的是 65 (距离12) 和 37 (距离16)。选 65。 (53 -> 65)
            2.  当前 65: 最近的是 67 (距离2)。选 67。 (65 -> 67)
            3.  当前 67: 最近的是 37 (距离30) 和 98 (距离31)。选 37。 (67 -> 37)
            4.  当前 37: 最近的是 14 (距离23)。选 14。 (37 -> 14)
            5.  当前 14: 剩下 98, 122, 124, 183。最近的是 98 (距离84)。选 98。 (14 -> 98)
            6.  当前 98: 剩下 122, 124, 183。最近的是 122 (距离24)。选 122。 (98 -> 122)
            7.  当前 122: 剩下 124, 183。最近的是 124 (距离2)。选 124。 (122 -> 124)
            8.  当前 124: 剩下 183 (距离59)。选 183。 (124 -> 183)
            服务序列: 53 -> 65 -> 67 -> 37 -> 14 -> 98 -> 122 -> 124 -> 183
        * **磁头移动距离**: (65-53) + (67-65) + (67-37) + (37-14) + (98-14) + (122-98) + (124-122) + (183-124)
            = 12 + 2 + 30 + 23 + 84 + 24 + 2 + 59 = **236 道**
        * **平均移动距离**: 236 / 8 = 29.5 道
        * **优点**: 显著改善了磁盘平均服务时间，吞吐量较高。
        * **缺点**: 可能导致**饥饿 (Starvation)** 现象。即如果不断有新的请求到达且其位置靠近当前磁头，那么距离较远的请求可能会长期等待得不到服务。

    * **(3) 扫描算法 (SCAN / Elevator Algorithm - 电梯算法)**:
        * **算法**: 磁头在一个方向上移动 (例如，从外圈到内圈) ，在移动过程中对遇到的所有访问请求进行服务。当到达该方向的末端 (或该方向上没有更多请求) 时，磁头改变移动方向，并继续服务。
        * **服务序列 (假设开始时磁头向磁道号减小的方向移动)**:
            1.  当前 53, 向0移动: 服务 37, 然后 14。 (53 -> 37 -> 14)
            2.  到达14 (假设是该方向最远请求或已到磁盘一端0), 改变方向向磁道号增大的方向移动:
            3.  服务 65, 67, 98, 122, 124, 183。 (14 -> 65 -> 67 -> 98 -> 122 -> 124 -> 183)
            服务序列: 53 -> 37 -> 14 -> 65 -> 67 -> 98 -> 122 -> 124 -> 183
        * **磁头移动距离**: (53-37) + (37-14) + (65-14) + (67-65) + (98-67) + (122-98) + (124-122) + (183-124)
            = 16 + 23 + 51 + 2 + 31 + 24 + 2 + 59 = **208 道** (注：幻灯片中为218，计算方式可能略有差异，如是否包含到磁盘物理边缘的移动。此处按服务完请求即转向计算)
        * **平均移动距离**: 208 / 8 = 26 道
        * **优点**: 克服了SSTF的饥饿问题，平均寻道时间较好，响应时间方差较小。
        * **缺点**: 对于刚被磁头越过的柱面的请求，需要等待磁头移到磁盘另一端再折回，等待时间较长。两端柱面的请求响应频率可能不均。

    * **(4) 单向扫描调度算法 (C-SCAN - Circular SCAN)**:
        * **算法**: 为了解决SCAN算法两端请求响应不均的问题。磁头总是从一个方向扫描到另一端 (例如，从0号柱面到最内层柱面) ，沿途服务请求。到达一端后，立即快速返回到起始端 (例如0号柱面) ，这个返回过程不服务任何请求。然后再从起始端开始下一次扫描。
        * **优点**: 提供了更均匀的等待时间。减少了新请求的最大延迟。
        * **缺点**: 返回时不服务，可能会有少量额外开销。

    * **(5) N步扫描策略 (N-step-SCAN)**:
        * **算法**:
            1.  将磁盘请求队列分成若干个长度为 N 的子队列。
            2.  按FCFS的顺序依次用SCAN算法处理这些子队列。
            3.  在处理某一个子队列时，新到达的请求必须添加到其他某个子队列中 (而不是当前正在处理的SCAN队列) 。
            4.  如果扫描到最后，剩下的请求数小于N，则它们都将在下一次扫描时 (作为一个子队列) 被处理。
        * **作用**: 克服SCAN算法中的“磁头臂的粘性” (Arm Stickiness) 问题。粘性指的是磁头可能会长时间停留在某个区域服务密集到达的请求，而使得其他区域的请求等待过久。N-step-SCAN通过分批处理，保证了磁头会周期性地扫过整个磁盘。
        * **特性**: 对于比较大的N值，其性能接近SCAN；当N=1时，N-step-SCAN退化为FIFO (FCFS)。

    * **(6) FSCAN 策略**:
        * **算法**:
            1.  使用两个子队列，Q1和Q2。
            2.  扫描开始时，所有当前存在的请求都在一个队列中 (例如Q1) ，而另一个队列 (Q2) 为空。
            3.  当磁头扫描并处理Q1中的请求时，所有新到达的请求都被放入Q2中。
            4.  当Q1处理完毕后，磁头开始处理Q2中的请求，同时新到达的请求放入Q1。如此交替。
        * **作用**: 同样是为了克服“磁头臂的粘性”。它确保了新请求不会无限期地延迟老请求的服务。

    * **(7) 旋转调度算法 (Rotational Scheduling)**:
        * **核心思想**: 在磁头到达目标柱面后，需要等待目标扇区旋转到磁头下方。旋转调度就是根据这个旋转延迟时间来决定同一柱面上多个请求的服务次序。
        * **分析与解决方案**:
            * **情况一**: 若干等待访问者请求访问**同一磁道 (同一柱面同一磁头)** 上的**不同扇区**。
                * **解决**: 总是让首先旋转到达读写磁头位置下的那个扇区先进行数据传送操作。 (最短旋转延迟优先)
            * **情况二**: 若干等待访问者请求访问**同一柱面不同磁头**对应的磁道上的**不同编号的扇区**。
                * **解决**: 同样考虑旋转位置，选择综合旋转延迟最短的那个请求。
            * **情况三**: 若干等待访问者请求访问**同一柱面不同磁头**对应的磁道上**具有相同的扇区号** (逻辑上，物理位置可能略有偏差)。
                * **解决**: 这些扇区 (几乎) 同时到达读写磁头位置下，此时可以任意选择一个读写磁头 (即任一磁道上的请求) 进行传送操作，或者根据其他策略 (如磁头切换时间) 。

    * **练习1 (同一柱面，不同磁头和扇区)**:
        * 请求: ①(柱面5, 磁头4, 扇区1), ②(柱面5, 磁头1, 扇区5), ③(柱面5, 磁头4, 扇区5), ④(柱面5, 磁头2, 扇区8)
        * **分析**: 所有请求都在柱面5。需要根据扇区号和可能的磁头切换时间来优化。假设扇区号按旋转顺序排列。
            * 如果当前磁头在磁头4，扇区0刚过：下一个是扇区1(①)，然后是扇区5(③)。
            * 如果可以快速切换磁头：
                * 若扇区按 1, 2, 3, 4, 5, 6, 7, 8 顺序旋转。
                * 当扇区1到达：服务请求① (H4,S1)。
                * 当扇区5到达：可以服务请求② (H1,S5) 或请求③ (H4,S5)。如果磁头切换成本低，且H1,S5 和 H4,S5 同时可达，则任选。
                * 当扇区8到达：服务请求④ (H2,S8)。
            * 实际最优次序取决于当前扇区位置、旋转方向、磁头切换时间。目标是最小化总的旋转延迟和磁头切换时间。

    * **练习2 (不同柱面，不同磁头和扇区，寻道+旋转)**:
        * 请求: ①(9,6,3), ②(7,5,6), ③(15,20,6), ④(9,4,4), ⑤(20,9,5), ⑥(7,15,2)
        * 当前磁头在柱面8。求最省时间的响应次序。
        * **一种最优响应顺序示例**: 8(起始) → 7 → 9 → 15 → 20
            1. **8 → 7**：磁头从8号柱面移动到7号柱面 (寻道距离1) ，优先服务7号柱面的请求。此处有请求②(7,5,6)和⑥(7,15,2)，根据旋转延迟选择先服务哪个 (如先服务扇区2的⑥，再服务扇区6的②) 。
            2. **7 → 9**：磁头从7号柱面移动到9号柱面 (寻道距离2) ，服务9号柱面的请求①(9,6,3)和④(9,4,4)，同样根据旋转延迟决定先后顺序。
            3. **9 → 15**：磁头移动到15号柱面 (寻道距离6) ，服务请求③(15,20,6)。
            4. **15 → 20**：磁头移动到20号柱面 (寻道距离5) ，服务请求⑤(20,9,5)。

### 2.6 信息的优化分布 (Optimized Information Layout on Disk)

* **问题**: 记录在磁道上的物理排列方式会影响顺序读写操作的效率。
* **例子场景**:
    * 一个处理程序要求按顺序处理文件中的8个逻辑记录。
    * 磁盘旋转一周为20毫秒 (假设每转处理完所有可见记录)。
    * 处理器处理一个记录需要5毫秒。

* **图示分析 (Interleaving - 交叉因子)**:
    * **无交叉 (Sequential Layout)**: 逻辑记录1, 2, 3, ... 存放在物理扇区 1, 2, 3, ...
        * 读取记录1 (假设1ms传输) -> 处理记录1 (5ms)。
        * 在处理记录1的5ms期间，磁盘继续旋转。当处理完毕准备读取记录2时，记录2所在的扇区可能已经转过磁头，需要等待几乎一整圈 (接近20ms) 才能再读到记录2。
        * 总时间 = 8 * (1ms读 + 5ms处理 + ~19ms等待) = 8 * 25ms = 200ms (估算)。
    * **有交叉 (Interleaved Layout)**: 为了匹配处理速度和磁盘旋转速度，逻辑记录的存放可以跳过一些物理扇区。
        * 例如，如果每处理一个记录磁盘转过N个扇区，那么逻辑记录i和逻辑记录i+1之间应该间隔N-1个物理扇区。
        * 假设5ms处理时间对应磁盘转过1/4圈 (5ms/20ms)。如果一圈有8个扇区 (简化) ，则转过2个扇区。那么交叉因子可以是2。
        * 记录1在扇区1。读1，处理1。此时磁盘可能到了扇区3附近。如果记录2放在扇区3，则可以立即读取。
        * **物理扇区应该如何安排**:
            * 逻辑记录: 1  2  3  4  5  6  7  8
            * 物理扇区: 1  4  7  2  5  8  3  6 (假设交叉因子为2，即跳过2个物理扇区)
        * **目标**: 当CPU处理完当前记录后，下一个逻辑记录所在的物理扇区正好旋转到磁头下，从而避免或减少旋转等待时间。

### 2.7 记录的成组与分解 (Record Blocking and Deblocking)

* **名词解释**:
    * **逻辑记录 (Logical Record)**: 用户或应用程序视角下的一个数据单元 (例如，数据库中的一条员工记录，一个文本文件中的一行) 。
    * **物理记录 (Physical Record / Block)**: 存储设备上一次I/O操作可以读写的最小单位 (例如，磁盘上的一个扇区或一个块) 。

* **记录的成组 (Blocking)**:
    * **定义**: 把若干个逻辑记录合并存放在一个物理块中的操作。
    * **实现**: 进行成组操作时，通常在内存中设置一个缓冲区，其长度等于逻辑记录长度乘以**块因子 (Blocking Factor)** (即一个物理块中包含的逻辑记录个数)。当缓冲区满或者文件结束时，将整个缓冲区 (一个物理块) 一次性写入外设。
    * **目的**:
        1.  **提高存储空间的利用率**: 如果逻辑记录很小，单独存放会浪费大量空间 (因为每个物理块都有固定开销，或者未填满) 。成组可以更充分地利用物理块。
        2.  **减少启动外设的次数，提高系统的工作效率**: 外设 (如磁盘) 的启动 (寻道、旋转) 非常耗时。通过一次读写多个逻辑记录 (一个物理块) ，可以显著减少I/O操作的次数。

* **记录的分解 (Deblocking)**:
    * **定义**: 从外设读入一个包含多个逻辑记录的物理块后，根据应用程序的需要，从中分离出单个逻辑记录的操作。
* **典型例子**:
    * **目录文件**: 目录文件本身可以看作是由多个目录项 (每个目录项是一个逻辑记录，包含文件名、属性、指向数据块的指针等) 组成的。这些目录项通常被成组存放在一个或多个磁盘块中。

### 2.8 RAID 技术 (Redundant Arrays of Independent Disks)

* **背景与动机**:
    * 不断增长的数据存储需求。
    * 对磁盘存储系统的**速度 (Performance)**、**容量 (Capacity)**、**容错 (Fault Tolerance)** 以及数据灾难发生后的**数据恢复 (Data Recovery)** 能力要求越来越高。

* **解决方案**: **RAID (独立磁盘冗余阵列)**
    * **定义**: 将多块独立的物理磁盘按照一定的要求组合起来，构成一个逻辑上的存储单元。操作系统将这个磁盘阵列看作是一个独立的存储设备 (一个大的逻辑磁盘) 。
    * **目标**: 实现高性能、高容量、高可靠性的存储技术。
    * **提出者**: 美国加州大学伯克利分校的 D.A. Patterson 教授等人于1988年提出。

* **RAID 技术的结构与基本思路**:
    * 通过把多个磁盘组织在一起，作为一个逻辑卷对外提供服务，实现**磁盘跨越 (Spanning)** 功能 (逻辑卷容量可以是多个磁盘容量之和)。
    * **数据组织存储方式**:
        1.  **数据分条 (Data Striping)**: 将数据分成多个数据块 (stripes/chunks)，并行地写入或读出到阵列中的多个磁盘上。
            * **作用**: 提高数据传输率，因为多个磁盘可以同时工作，总带宽增加。
        2.  **冗余 (Redundancy)**: 通过**镜像 (Mirroring)** 或**数据校验 (Data Parity)** 操作，在磁盘阵列中存储额外的信息，用于在某个磁盘发生故障时恢复数据。
            * **作用**: 提高容错能力和系统的可用性、可靠性。

* **RAID 级别 (RAID Levels)**: 不同的数据组织和冗余方式构成了不同的RAID级别。

    * **RAID 0 – 条带化 (Striping)**
        * **原理**: 数据被分割成条带，轮流分布在阵列中的所有磁盘上 (例如，块1存磁盘1，块2存磁盘2，块3存磁盘1...) 。
        * **操作**: 当有数据请求时，可以同时从多个磁盘并行读取或写入各自的条带。
        * **优点**:
            * 充分利用总线带宽，数据吞吐率显著提高 (理论上是单个磁盘的N倍，N为磁盘数) 。
            * 驱动器负载均衡。
            * **性能最佳** (在所有RAID级别中，如果不考虑冗余带来的开销)。
        * **缺点**: **无冗余** (即无差错控制或容错能力)。阵列中任何一个磁盘损坏都会导致所有数据的丢失。
        * **适用**: 对性能要求极高，但对数据可靠性要求不高的场景 (如临时数据、视频编辑中间文件) 。

    * **RAID 1 – 镜像 (Mirroring)**
        * **原理**: 将所有数据完全相同地同时写入两块 (或多块) 磁盘。这两块磁盘互为镜像。
        * **优点**:
            * **数据安全性最好**。最大限度保证数据安全及可恢复性。任何一块磁盘损坏，数据仍然可以从另一块镜像磁盘获得。
            * 读取性能可能有所提升 (可以从两块盘中任选一块读取，或并发读) 。
        * **缺点**:
            * **磁盘利用率低**，仅为50% (以两块盘为例) 。成本较高。
            * 写入性能可能略有下降或持平，因为数据要写两遍。
        * **适用**: 对数据可靠性要求极高的场景 (如操作系统盘、数据库) 。

    * **RAID 2 – 并行访问 — 海明码校验 (Hamming Code Parity)**
        * **原理**: 将数据以位 (bit) 或字节 (byte) 为单位进行条带化分布于不同硬盘。使用海明码 (Hamming Code) 作为纠错码，并将校验码也分散存储在专门的校验盘上。
        * **特点**:
            * 可以检测并纠正单位错误。
            * 数据存取时，整个磁盘阵列中所有数据盘和校验盘一起动作，在各个磁盘的相同位置 (同步主轴) 平行存取，所以理论上有很好的存取时间。
        * **缺点**: 实现复杂，成本高 (需要多个校验盘) ，海明码对于磁盘这种大块错误来说效率不高。
        * **现状**: 商业上很少使用，已被RAID 3, 4, 5等取代。

    * **RAID 3 – 交错位奇偶校验 (Bit-interleaved Parity) / 字节级条带与专用奇偶校验盘**
        * **原理**: 类似RAID 2，通常以字节 (byte) 为单位将数据拆分，并交叉写入数据盘。设置一个专门的磁盘作为**校验盘 (Parity Disk)**，用于存储所有数据盘对应位置字节的奇偶校验信息。
        * **特点**:
            * 如果某个数据盘损坏，可以通过其他数据盘和校验盘的数据异或运算来恢复丢失数据。
            * 对于大量连续数据的传输 (如大文件读写) 性能较好。
        * **缺点**:
            * 校验盘是瓶颈，特别是对于大量随机的小写入操作，每次写入都需要更新校验盘。
            * 一个磁盘损坏时，性能会下降。
        * **现状**: 也较少独立使用，通常被RAID 5等更优方案替代。

    * **RAID 4 – 交错块奇偶校验 (Block-interleaved Parity) / 块级条带与专用奇偶校验盘**
        * **原理**: 与RAID 3相似，但是以数据块 (block) 为单位进行条带化，而不是字节。仍然使用一个专用的奇偶校验盘。
        * **优点**: 对于随机读操作，如果读取的数据块在一个盘上，则可以独立进行，并发性比RAID 3好。
        * **缺点**: 写操作仍然是瓶颈。任何数据块的写入都需要读取旧数据块、旧校验块，计算新校验块，然后写入新数据块和新校验块 (读-修改-写过程) ，并且所有校验更新都集中在校验盘上。

    * **RAID 5 – 交错块分布式奇偶校验 (Block-interleaved Distributed Parity)**
        * **原理**: 与RAID 4类似，以数据块为单位进行条带化。但关键区别在于，**奇偶校验信息不是存储在专用校验盘上，而是分布式地存储在阵列中的所有磁盘上** (例如，第一个条带的校验在磁盘N，第二个在磁盘N-1，以此类推，循环分布) 。
        * **优点**:
            * 克服了RAID 4的校验盘瓶颈问题，因为校验写入也分散到各个磁盘，提高了并发写性能。
            * 数据读出效率高 (可以并行从多个数据盘读) 。
            * 磁盘利用率较好 (N-1)/N，N为磁盘总数。
            * 提供了较好的数据可靠性 (允许一块磁盘损坏) 。
        * **缺点**:
            * **写损失 (Write Penalty)**: 每个写操作仍然需要进行“读-修改-写”：读取旧数据、读取旧校验、计算新校验、写入新数据、写入新校验。对于小写操作，这可能涉及4次磁盘I/O。
            * 控制器设计复杂。
            * 磁盘损坏时，重构数据会消耗较多系统资源，性能下降。
        * **适用**: 兼顾性能、容量和可靠性的通用选择，如文件服务器、应用服务器。

    * **RAID 6 – 交错块双重分布式奇偶校验 (Block-interleaved Dual Distributed Parity)**
        * **原理**: 在RAID 5的基础上，增加了第二个独立的奇偶校验方案 (例如，使用两种不同的校验算法，或者一个P校验一个Q校验) ，并将这两组校验信息也分布式地存储在所有磁盘上。
        * **优点**:
            * **数据恢复能力极强**。可以容忍阵列中任意**两块磁盘同时损坏**而不丢失数据。
            * 可靠性远高于RAID 5。
        * **缺点**:
            * **写损失更严重**: 因为要计算和写入两组校验信息，写入性能比RAID 5更差。
            * 磁盘利用率进一步降低 (N-2)/N。
            * 控制器设计更为复杂，成本更高。
        * **适用**: 对数据可靠性和可用性要求极高，且能容忍一定写入性能损失的场景，如长期归档、关键业务数据。

    * **RAID 7 – 最优化异步高I/O速率及高数据传输率 (Proprietary RAID Level)**
        * **注意**: RAID 7 不是一个业界标准，而是一家名为Storage Computer Corporation的公司提出的专有RAID实现。
        * **特点**:
            * 自身带有智能化的实时操作系统和用于存储管理的软件工具，可以独立于主机运行。
            * 每个磁盘通常有独立的I/O通道，与主缓存通道连接。
            * 其内置操作系统可以直接对每个磁盘的访问进行控制，允许每个磁盘在不同的时段独立进行数据读写，具有很高的并发性。
            * 通常有高速缓存。
        * **缺点**: 价格非常昂贵，是专有技术，缺乏通用性。

    * **对比RAID 0 ~ RAID 7的效率以及在数据安全性上的优劣 (简要总结)**:
        * **RAID 0**: **效率最高** (读写速度快，磁盘利用率100%)；**安全性最差** (无冗余，单盘故障即数据丢失)。
        * **RAID 1**: 读效率尚可/好，写效率一般/略低；**安全性非常好** (完全镜像，单盘故障不影响，利用率50%)。
        * **RAID 2**: 理论效率高 (并行存取) ；安全性一般 (海明码纠错) ；**实际已不用**。
        * **RAID 3**: 顺序读写效率高，随机读写效率差 (校验盘瓶颈) ；安全性一般 (单盘冗余) ；**实际较少用**。
        * **RAID 4**: 随机读效率优于RAID3，写仍有校验盘瓶颈；安全性一般 (单盘冗余) ；**实际较少用**。
        * **RAID 5**: 读效率好，写效率一般 (分布式校验，但有写惩罚) ；**安全性较好** (单盘冗余，利用率(N-1)/N) 。**常用**。
        * **RAID 6**: 读效率好，写效率较低 (双校验，写惩罚更大) ；**安全性非常好** (双盘冗余，利用率(N-2)/N) 。**对安全性要求高时用**。
        * **RAID 7 (专有)**: 宣称效率非常高；安全性也高；但**成本极高且非标准**。

* **RAID 嵌套 (Nested RAID / Hybrid RAID)**: 组合不同的RAID级别以获得特定优势。
    * **RAID 0+1 (或 RAID 01) - 条带化的镜像 (Stripe of Mirrors)**:
        * 先将磁盘两两配对做RAID 1 (镜像) ，得到多个镜像对。
        * 然后将这些镜像对作为基本单元再进行RAID 0 (条带化) 。
        * **优点**: 性能好 (得益于RAID 0) ，有冗余 (得益于RAID 1) 。
        * **缺点**: 如果一个镜像对中的一块盘坏了，整个镜像对仍工作；但如果之后该镜像对的另一块盘也坏了，则整个RAID 0条带失效。价格较高。
    * **RAID 1+0 (或 RAID 10) - 镜像化的条带 (Mirror of Stripes)**:
        * 先将磁盘分组做RAID 0 (条带化) ，得到多个条带集。
        * 然后将这些条带集两两配对进行RAID 1 (镜像) 。
        * **优点**: 性能好，冗余性好。容错能力通常优于RAID 0+1 (只要每个条带集至少有一份镜像是好的，数据就不会丢) 。
        * **缺点**: 价格高，至少需要4块磁盘。磁盘利用率50%。
        * **RAID 10 通常被认为比 RAID 01 更可靠且性能更好或相当，是高性能高可靠的热门选择。**

* **Acknowledgement**: 清华大学 向勇、陈渝 (感谢原课件作者)

---

## 3. 文件系统结构设计

涉及文件系统的逻辑模型、内部组织以及如何与多种具体文件系统共存。

### 3.1 文件系统分类

* **磁盘文件系统 (Disk File Systems)**: 为传统旋转磁盘设计。
    * 实例系统: FAT (File Allocation Table), NTFS (New Technology File System), ext2/3/4 (Linux Extended File System), ISO9660 (CD/DVD标准) 等。
* **Flash文件系统 (Flash File Systems)**: 专门为闪存 (如SSD, U盘, SD卡) 的特性 (如擦写次数限制、无寻道时间、按块擦除) 优化设计。
    * 实例系统: F2FS (Flash-Friendly File System), JFFS2, YAFFS。
* **数据库文件系统 (Database File Systems)**: 将文件系统的元数据甚至数据本身存储在数据库中，利用数据库的能力进行管理。
    * 实例系统: WinFS (Windows Future Storage - 项目已中止)。
* **日志文件系统 (Journaling File Systems)**: (已在前面一致性部分提到) 通过记录元数据变更日志来提高崩溃恢复能力。如NTFS, ext3/4。
* **网络/分布式文件系统 (Network/Distributed File Systems)**: 允许通过网络访问存储在远程服务器上的文件，使用户感觉像访问本地文件一样。
    * 实例系统: NFS (Network File System), SMB (Server Message Block, 也称CIFS), AFS (Andrew File System), GFS (Google File System)。
* **虚拟文件系统 (Virtual File Systems)**: (后面会详述) 提供一个统一的接口给上层应用，使其可以透明地访问多种不同类型的底层具体文件系统。

### 3.2 分布式文件系统 Ceph

* **设计者**: Sage Weil (美国加州大学Santa Cruz分校博士论文，DreamHost联合创始人)。
* **背景**:
    * 2007年毕业后，Sage开始全职投入到Ceph开发，目标是使其能适用于生产环境。
    * **主要目标**: 设计成一个基于POSIX标准、**没有单点故障**的分布式文件系统，数据能实现**容错**和**无缝复制**。
* **里程碑**: 2010年3月，Linus Torvalds将Ceph客户端合并到Linux内核2.6.34中。
* **特色**:
    * 强大的容错实现机制 (如CRUSH算法进行数据分布和复制)。
    * 简化海量数据管理的功能 (如动态扩展、自我修复)。
    * 提供对象存储、块存储和文件系统接口。

### 3.3 华为 EROFS (Extendable Read-Only File System)

* **背景**:
    * 2018年6月，华为工程师在开源社区展示了基于Linux的全新**只读**文件系统EROFS。
    * **特点**: 采用了改进的压缩算法，目标是在保证节省存储空间的同时，还提高性能和速度。
    * **早期测试数据**: EROFS随机数据读取性能对比ext4有明显优势，最高超过500%。
* **发布**: 2019年3月，华为在P30系列手机发布会上正式推出了“EROFS超级文件系统”。
* **优势 (应用于手机等嵌入式设备)**:
    * **随机读取性能提升**: 平均提升20%，最大可提升近300%。
    * **系统ROM空间占用节省**: 例如，P30 Pro 128G版本可节省2GB (不同机型节省空间会有差异) 。
    * **改善卡顿**: 可避免在内存紧张时因低效地反复读压缩数据、解压缩数据带来的整机卡顿问题。
    * **安全性增强**: 天然只读设计，使得系统分区不可被第三方应用改写，增强了系统的完整性和安全性。

### 3.4 ReFS (Resilient File System) 文件系统

* **引入**: Windows Server 2012 中首次引入。
* **主要面向场景**: 企业级环境和大规模存储系统，如文件服务器、存储区域网络 (SAN) 和备份解决方案。
* **设计目标**: 提供高度的**可靠性 (Reliability)**、**数据完整性 (Data Integrity)** 和 **扩展性 (Scalability)**，以满足大规模存储和数据管理的需求。
* **注意**: ReFS 并不适用于所有场景 (例如，它不能作为Windows的启动分区，早期版本功能也不如NTFS全面，但后续版本在不断增强) 。

* **ReFS 的一些关键特性**:
    * **数据完整性 (Data Integrity)**:
        * 使用“**完整性流 (Integrity Streams)**”技术来保证数据的完整性。
        * 会对每个文件 (或元数据) 计算并存储校验和 (checksum)。
        * 在读取文件时验证校验和，以检测和 (如果可能) 自动修复数据损坏 (例如，如果与镜像数据或奇偶校验数据一起使用)。
    * **故障容错 (Fault Tolerance)**:
        * 支持多种故障容错功能，包括与存储空间 (Storage Spaces) 结合使用实现数据**镜像 (Mirroring)** 和数据**奇偶校验冗余 (Parity)**。
        * 可以将数据分布在多个磁盘上，以防止单点故障，并在发生磁盘故障时提供自动修复和数据恢复功能。
    * **可扩展性 (Scalability)**:
        * ReFS设计为支持非常大的卷容量 (远超NTFS) 和大量文件，以及高性能工作负载。
    * **支持稳定写入 (Resilient Writes - "Allocate on Write")**:
        * 采用“**写时分配 (Allocate on Write)**”或称为“写时复制 (Copy-on-Write)”的技术。
        * 在进行写操作时，不直接修改原始数据块，而是将新数据写入到新的磁盘位置，然后更新元数据指向新位置。
        * **优点**: 保证写操作的原子性和数据一致性。如果写入过程中发生断电等故障，原始数据仍然完好无损。
    * **兼容性 (Compatibility)**:
        * ReFS 与 NTFS 文件系统在某些方面兼容 (例如，部分API) 。
        * 提供逐步迁移的能力。它可以与现有的NTFS卷一起使用，用户可以逐步将数据或卷迁移到ReFS，而无需立即重新格式化所有磁盘。

### 3.5 文件系统设计问题

在设计一个文件系统时，需要考虑以下核心问题：

1.  **如何定义文件系统对用户的接口？**
    * **文件及属性 (Files and their Attributes)**: 文件名、类型、大小、时间戳、所有者、权限等。
    * **文件操作 (File Operations)**: 创建、删除、打开、关闭、读、写、移动、重命名等。
    * **目录结构 (Directory Structure)**: 如何组织文件 (如层次结构、平面结构) ，目录操作 (创建、删除、列出等) 。
2.  **如何将逻辑文件系统映射到物理磁盘设备上？**
    * **数据结构 (Data Structures)**: 用于管理文件元数据 (如i-node、FAT表、MFT) 、目录、空闲空间 (如free-space bitmap) 的数据结构。
    * **算法 (Algorithms)**: 用于查找文件、分配/回收磁盘块、目录管理的算法。
3.  **文件系统实现时如何分层？**
    * 将复杂的系统分解为多个层次，每层提供特定的功能，并依赖下层服务。

### 3.6 文件系统通用模型 (层次模型)

一个典型的文件系统可以看作是一个层次结构模型：

```
\+-------------------------+
|       应用程序            |  (User Applications)
\+-------------------------+
^
| (文件系统接口调用，如 open, read, write)
\+-------------------------+
|     逻辑文件系统层         |  (Logical File System Layer)
\+-------------------------+
^
| (文件名 -\> 文件内部标识)
\+-------------------------+
|    文件组织模块层          |  (File Organization Module Layer)
\+-------------------------+
^
| (文件逻辑块号 -\> 磁盘物理块号)
\+-------------------------+
|     基本文件系统层         |  (Basic File System Layer)
\+-------------------------+
^
| (读/写物理块命令)
\+-------------------------+
|      基本I/O控制层        |  (Basic I/O Control Layer / Device Drivers)
\+-------------------------+
^
| (硬件指令)
\+-------------------------+
|        物理磁盘           |  (Physical Disk)
\+-------------------------+

```

* **各层的作用**:
    * **文件系统接口 (Application Programming Interface - API)**:
        * 定义了一组供应用程序员使用的、用于操作文件和目录的方法 (如 `open()`, `read()`, `write()`, `close()`, `mkdir()` 等) 。
    * **逻辑文件系统层 (Logical File System Layer)**:
        * 管理文件系统的元数据，如目录结构、文件名与文件内部标识符 (如i-node号或文件控制块FCB) 的映射。
        * 负责文件的保护和安全检查 (访问权限验证) 。
        * 将符号化的文件名转换为文件组织模块可以使用的内部表示。
    * **文件组织模块层 (File Organization Module Layer / Allocation Module)**:
        * 负责将文件的逻辑块号 (应用程序或逻辑文件系统看到的块号) 映射到磁盘上的物理块号。
        * 管理磁盘空间的分配与回收 (知道哪些块是空闲的，哪些块属于哪个文件) 。
        * 处理文件的具体数据块的读写请求。
    * **基本文件系统层 (Basic File System Layer / Device I/O Layer)**:
        * 接收来自文件组织模块的、针对特定物理块的读写请求。
        * 主要向相应的设备驱动程序发出读写磁盘物理块的一般命令 (例如，读取设备X上的Y号块到内存地址Z) 。
    * **基本I/O控制层 (Basic I/O Control Layer / Device Drivers and Interrupt Handlers)**:
        * 由设备驱动程序 (Device Drivers) 和中断处理程序 (Interrupt Handlers) 组成。
        * 设备驱动程序将基本文件系统层发来的通用命令转换为特定磁盘控制器可以理解的具体硬件指令。
        * 负责实际控制硬件设备，实现内存和磁盘系统之间的信息传输，并处理磁盘操作完成时产生的中断。

### 3.7 虚拟文件系统 (Virtual File System - VFS)

* **定义**: 对多个**不同类型**的底层具体文件系统 (如ext4, NTFS, FAT32, NFS等) 的一个**抽象层**。
* **功能**:
    1.  **提供相同的文件和文件系统接口**: 为上层应用程序和操作系统内核的其他部分提供一个统一的、与具体文件系统无关的接口。应用程序使用标准的系统调用 (如`open`, `read`, `write`) ，而无需关心底层文件系统的具体类型。
    2.  **管理所有文件和文件系统关联的数据结构**: VFS维护一些通用的、描述已挂载文件系统和已打开文件的数据结构 (如vnode/inode结构，dentry结构) 。
    3.  **高效查询例程，遍历文件系统**: 提供通用的方法来查找文件、解析路径名等，这些操作最终会分派到具体文件系统的实现。
    4.  **与特定文件系统模块的交互**: VFS定义了一套规范，每个具体的文件系统 (如ext4模块) 需要按照这套规范实现一组操作函数 (如`vfs_read_inode`, `vfs_lookup`等) 。当VFS收到一个操作请求时，它会根据文件所在的具体文件系统类型，调用该文件系统注册的相应操作函数。

* **VFS 在系统中的位置 (示意图)**:

    ```
    +-------------------------------------+
    |  用户态 (User Mode)                  |
    |   库函数/API (open(), read(), ...)   |
    +-------------------------------------+
                | 系统调用 (System Call)
    +-------------------------------------+
    | 内核态 (Kernel Mode)                 |
    |   系统调用接口 (sys_open(), ...)      |
    |             |                       |
    |   +-----------------------------+   |
    |   |  虚拟文件系统 (VFS)           |   |
    |   | (vfs_open(), vfs_read(), .) |   |
    |   +-----------------------------+   |
    |       /      |      \      \        |
    | +-------+ +-------+ +-------+ +-----+
    | |MS-DOS | | EXT2  | | EXT3  | |/proc|  (具体文件系统实现)
    | +-------+ +-------+ +-------+ +-----+
    +-------------------------------------+
    ```

### 3.8 日志结构文件系统 (Log-structured File System - LFS)

* **背景**: 传统文件系统写操作可能涉及多次磁盘寻道 (例如，更新i-node、更新目录项、写入数据块，这些可能在磁盘的不同位置) 。
* **问题定位**: 如何提高磁盘**写操作**的效率？ (假设读操作大部分可以由文件缓存满足) 。
* **核心思路**:
    * **避免为写操作寻找特定位置**: 不再将数据和元数据写回到它们原来的固定位置。
    * **将整个磁盘 (或分区) 看作是一个日志 (Log)**: 所有写操作 (包括数据块和元数据块如i-node、目录块) 都被顺序地、追加地写入到日志的末尾，就像写日志一样。
    * **集中写入 (Segmented Writes)**: 通常会将多个小的写操作缓冲起来，聚合成一个大的段 (Segment)，然后一次性将整个段写入日志末尾。这使得写操作主要是顺序写，非常高效。
* **实现特点**:
    * **i-node 和文件内容一起写入**: 当文件数据或元数据被修改时，新的数据块和更新后的i-node块都被写入日志末尾。
    * **i-node 表 (i-node Map)**: 由于i-node的位置会不断变化 (每次修改都会写到新的位置) ，需要一个i-node map来记录每个i-node的最新位置。i-node map本身也需要持久化，并可能分段存储在日志中。
    * **清理线程/垃圾回收 (Cleaner/Garbage Collector)**:
        * 日志会不断增长并最终填满磁盘。清理线程负责扫描日志，识别哪些块是“死的” (即不再被任何活动i-node引用，是旧版本的数据或元数据) 。
        * 将日志中仍然“活”的块 (属于最新版本的文件) 收集起来，并重新写入到新的、干净的日志段中，从而回收旧日志段占用的空间，形成新的空闲空间供后续写入。这个过程称为**清理 (Cleaning)** 或 **垃圾回收 (Garbage Collection)**。

### 3.9 日志文件系统 (Journaling File System)

* **与LFS的关系**: 借鉴了日志结构文件系统的“先写日志再操作”的核心思想，但主要目的是为了**鲁棒性 (Robustness)** 和快速崩溃恢复，而不是极致的写性能 (尽管也可能带来性能改善) 。
* **核心机制**:
    1.  **保存一个日志 (Journal/Log)**: 在对文件系统的元数据 (有时也包括文件数据，取决于日志模式) 进行实际修改之前，先将描述这些将要进行的修改操作的一个或多个**日志项 (Log Entry)** 写入磁盘上的一个专用日志区域。
    2.  **原子事务 (Atomic Transaction)**: 一系列相关的修改 (如创建一个文件可能涉及分配i-node、写入目录项、分配数据块) 被视为一个原子事务。
    3.  **日志提交 (Log Commit)**: 当事务的所有日志项都安全写入磁盘后，会写入一个提交记录。
    4.  **实际修改 (Checkpointing/Applying to Filesystem)**: 然后才将这些修改应用到文件系统的实际位置 (如i-node表、数据块区) 。
    5.  **系统出错后恢复 (Recovery)**:
        * 如果系统在修改应用到文件系统之前但在日志提交之后崩溃，重启时，文件系统会检查日志。
        * 通过**回放 (Replaying)** 日志中已提交但可能未完全应用到文件系统的操作，来完成这些操作，确保文件系统达到一致状态。
        * 如果事务未提交就崩溃，则这些操作被视为未发生，日志中的相关条目会被忽略或回滚。
* **典型步骤 (例如，在目录中删除一个文件)**:
    * **不使用日志的步骤**:
        1.  从目录中删除文件的目录项。
        2.  释放文件的i-node到空闲i-node池。
        3.  将文件占用的所有磁盘块归还到空闲磁盘块池。
        * 如果在这些步骤之间崩溃，可能导致目录项已删但i-node和数据块未释放 (资源泄漏) ，或i-node已释放但目录项还在 (悬空指针) 等不一致状态。
    * **使用日志的步骤**:
        1.  **写日志项**: 记录将要执行的三个操作 (删除目录项、释放i-node、归还数据块) 到日志中。
        2.  **把日志项写入磁盘 (Flush Log to Disk)**: 确保日志安全落盘。
        3.  **执行操作**: 实际修改目录、i-node池和空闲块池。
        4.  **擦除日志项 (或标记为已完成/Checkpoint)**: 当所有操作都安全地反映到主文件系统结构后，对应的日志项就不再需要用于恢复了。
* **例子**: Windows的NTFS，Linux的Ext3、Ext4、ReiserFS，macOS的HFS+ (with Journaling), APFS。

### 3.10 XV6：日志系统 (XV6 Logging System Example)

* **相关头文件**: `#include <kernel/log.c>` (通常实现放在 .c 文件)
* **设计原则**:
    * 当没有活跃的文件系统相关的系统调用 (fs syscalls) 时，才进行日志的**提交 (Commit)**。
    * 这样做是为了确保磁盘上的日志一定不包含尚未在内存中完成 (但未提交) 的写入操作，保证了原子性。
* **日志区域结构 `struct log`**: (通常在 `log.h` 或 `fs.h` 中定义)
    * `lock`:保护日志结构的锁。
    * `start`: 日志区域在磁盘上的起始块号。
    * `size`: 日志区域占用的总块数。
    * `n`: 当前日志中已记录的 (未提交的) 块的数量。
    * `dev`: 日志所在的设备号。
    * `lh` (`struct logheader`): 内存中的日志头部，通常包含一个数组，记录了当前事务中所有被修改的块的实际磁盘块号。
        * `logheader` 缓存文件系统写入操作的日志块信息，随后这些日志块 (副本) 和 `logheader` 本身会作为事务的一部分被批量提交到磁盘上的日志区域。
* **关键函数**:
    * `begin_op()`:
        * 标志一个文件系统操作序列 (事务) 的开始。
        * 需要获取日志锁。
        * 如果日志已满或正在提交，则可能需要等待。
    * `log_write(struct buf *b)`:
        * 在事务中，当一个内存中的数据块 `b` (它对应磁盘上的某个实际数据块) 被修改后，调用此函数。
        * 此函数会将块 `b` 的内容复制到日志区域的一个槽中 (如果它还没在当前事务的日志中) ，并记录其原始磁盘块号到 `logheader`。它并不立即写磁盘。
    * `end_op()`:
        * 标志一个文件系统操作序列 (事务) 的结束。
        * 调用 `commit()` 来将当前事务的所有日志条目写入磁盘。
        * 释放日志锁。
    * `commit()`: (XV6 中通常在 `end_op` 内部调用)
        1.  `write_log()`: 将所有在 `logheader` 中记录的、被修改的数据块的副本 (这些副本之前已暂存在内存的日志槽位) 写入到磁盘上日志区域的相应数据块位置。
        2.  `write_head()`: 将 `logheader` 本身 (包含了哪些块被写入日志的元数据) 写入到磁盘上日志区域的头部。这是提交点。一旦头部成功写入，事务就被认为是已提交的。
        3.  `install_trans()`: 将日志区域中已提交的数据块 (副本) 复制到它们在文件系统中的最终目标位置。
        4.  `清空日志头/写零日志头`: 更新磁盘上的日志头，表示事务已成功应用到文件系统，日志空间可重用。
* **日志系统恢复 `recover_from_log()`**:
    * 在系统启动时调用。
    * `read_head()`: 读取磁盘上日志区域的头部，获取上次记录的需要重做的块的信息 (即上次已提交但可能未完全 `install_trans` 的事务) 。
    * `install_trans()`: 根据日志头的信息，重放这些块的修改，将日志中的数据块内容写入到它们在文件系统中的目标地址。
    * `清空日志区域/写零日志头`: 完成恢复后，清空日志头，表示日志是干净的。

### 3.11 拓展：Log-Structured (日志结构思想的应用)

* 日志结构的“追加写”思想不仅在文件系统中得到运用，在其他数据存储和管理领域也有类似运用，主要是为了将随机写转换为顺序写以提高写性能。
* **Log-Structured Merge-Tree (LSM Tree)**:
    * 一种数据结构，广泛应用于现代NoSQL数据库 (如 LevelDB, RocksDB, Cassandra, HBase) 。
    * **核心思想**:
        * 写入操作首先进入内存中的一个有序数据结构 (如 memtable，通常是平衡树或跳表) 。
        * 当 memtable 达到一定大小时，将其以有序的形式刷到磁盘上，形成一个不可变的、有序的段文件 (SSTable - Sorted String Table)。这通常是顺序写。
        * 磁盘上会有多个这样的SSTable。查询时可能需要查找多个SSTable。
        * 后台会定期进行**合并 (Compaction)** 操作，将多个小的、旧的SSTable合并成大的、新的SSTable，同时处理掉已删除或被覆盖的数据，减少查询时的文件数量。
    * **优点**: 极大地提高了写吞吐量，因为写操作主要是内存操作和顺序磁盘写。
    * 常配合**预写日志 (Write-Ahead Log - WAL)** 运用：在数据写入 memtable 之前，先将操作写入 WAL。如果系统崩溃，可以通过 WAL 恢复 memtable，保证数据不丢失。
* **应用实例**: LevelDB, RocksDB。

---
## 4. Windows 文件系统模型

Windows 操作系统有一个复杂且分层的I/O系统，文件系统是其中的一个重要组成部分。

```

\+--------------------------------------+
|             应用程序 (User Mode)     |
|             (I/O API calls)          |
\+--------------------------------------+
|
\+--------------------------------------+
|           NT 执行体 (Kernel Mode)    |
|                                      |
|  +-----------------+                 |
|  |   I/O 管理器    |\<----------------+ (IRP生成与分发)
|  | (I/O Manager)   |                 |
|  +-----------------+                 |
|       |         ^                    |
|       |(IRP)    |(IRP)               |
|       V         |                    |
|  +-----------------+  (可选)         |
|  | 过滤驱动程序    |                 |  (如杀毒软件、加密驱动)
|  | (Filter Drivers)|                 |
|  +-----------------+                 |
|       |         ^                    |
|       V         |                    |
|  +-----------------+                 |
|  | 文件系统驱动程序|                 |  (FSD - e.g., NTFS.SYS, FASTFAT.SYS)
|  | (File System    |                 |
|  |  Drivers)       |                 |
|  +-----------------+                 |
|       |         ^                    |
|       V         |                    |
|  +-----------------+  (可选)         |
|  | 过滤驱动程序    |                 |  (如下层卷管理驱动)
|  | (Filter Drivers)|                 |
|  +-----------------+                 |
|       |         ^                    |
|       V         |                    |
|  +-----------------+                 |
|  |  设备驱动程序   |                 |  (如磁盘驱动程序 Disk.sys)
|  | (Device Drivers)|                 |
|  +-----------------+                 |
|       |         ^                    |
|       V         |                    |
|  +-----------------+                 |
|  |硬件抽象层 (HAL) |                 |
|  | (Hardware       |                 |
|  | Abstraction Lyr)|                 |
|  +-----------------+                 |
|           |                          |
\+--------------------------------------+
|
\+--------------------------------------+
|           物理设备 (Hardware)        |
|           (e.g., Disk, Network)      |
\+--------------------------------------+

```

* **I/O 管理器 (I/O Manager)**:
    * NT执行体中的核心组件，负责处理所有设备的I/O操作。
    * 为用户模式的应用程序提供统一的I/O API。
    * 创建和管理 **I/O 请求包 (IRP - I/O Request Packets)**，IRP是描述I/O操作的数据结构。
    * 将IRP分派给相应的驱动程序栈进行处理。
    * 管理驱动程序的加载和卸载。
* **驱动程序 (Drivers)**:
    * **设备驱动程序 (Device Drivers)**:直接与硬件设备通信，控制硬件操作。
    * **文件系统驱动程序 (FSD - File System Drivers)**: 实现特定文件系统的逻辑 (如NTFS, FAT) 。将面向文件的请求转换为对存储设备的更底层的请求。
    * **过滤驱动程序 (Filter Drivers)**: 可以插入到驱动程序栈的多个位置 (设备驱动之上、FSD之上或之下) ，用于拦截和修改IRP，以提供附加功能，如加密、压缩、病毒扫描、监控等。
        * **注册 (Registration)**: 过滤驱动程序会向I/O管理器注册，依附到特定的设备或驱动程序栈上。
* **硬件抽象层 (HAL - Hardware Abstraction Layer)**:
    * 隔离操作系统内核和具体硬件平台 (如不同的主板、处理器架构) 的差异。使得Windows内核更具可移植性。

### 4.1 FSD (文件系统驱动程序)

* **分类**:
    * **本地FSD (Local FSDs)**: 允许用户访问连接到本地计算机的存储设备上的数据 (如NTFS.SYS, FastFAT.SYS) 。
    * **远程FSD (Remote FSDs / Redirectors)**: 允许用户通过网络访问存储在远程计算机上的数据 (如 RDBSS.SYS 配合 MUP.SYS 和具体的网络小型重定向器如 mrxsmb.sys (SMB/CIFS)) 。
* **作用**: Windows文件系统的所有相关操作都通过相应的FSD来完成。FSD负责解析文件系统结构、管理元数据、分配空间、实现文件级安全等。

* **FSD 与 虚拟内存管理器(VMM) 和 高速缓存管理器(Cache Manager) 的交互 (示意图解释)**:
    * **关键组件**:
        * **虚拟内存管理器 (VMM)**: 负责管理系统的虚拟地址空间、分页、缺页处理等。
        * **高速缓存管理器 (Cache Manager)**: 负责文件数据的系统级缓存。
        * **文件系统驱动程序 (FSD)**:
        * **存储设备驱动程序 (Storage Device Driver)**:
    * **交互场景**:
        * **用户发起的显式文件I/O**:
            * `NtReadFile()` / `NtWriteFile()` (内核API): 用户模式的 `ReadFile`/`WriteFile` 会调用这些。
            * 如果文件是缓存的，I/O请求会通过 **Cache Manager**。
                * `CcCopyRead()`: 从缓存读取数据。如果数据不在缓存，Cache Manager会从FSD请求数据。
                * `CcCopyWrite()`: 数据写入缓存。
            * 如果文件是非缓存的，I/O请求直接发给FSD。
            * FSD将文件级请求转换为对存储设备驱动程序的块级请求。
        * **缺页中断 (Page Fault) / 内存映射文件**:
            * 当进程访问一个尚未在物理内存中的虚拟页面时 (例如，访问内存映射文件的页面) ，会发生缺页中断。
            * **缺页事件处理机 (MmAccessFault in VMM)** 会介入。
            * 如果缺页的页面属于一个文件 (如内存映射文件或可执行文件代码段) ，VMM会请求 **Cache Manager** 或直接通过FSD (`IoPageRead()`) 从磁盘加载该页面。
        * **高速缓存提前读 (Cache Prefetching)**:
            * **Cache Manager** (`CcReadAhead()`) 可能会根据访问模式主动从FSD预读文件数据到缓存中。
        * **高速缓存延迟写 (Cache Lazy Write)**:
            * 修改后的缓存页面 (脏页) 不会立即写回磁盘。
            * **Cache Manager** 的**延迟写线程 (Lazy Writer)** 或 **内存管理器的修改页面写入器 (Modified Page Writer)** (`MmFlushSection()`, `IoAsynchronousPageWrite()`) 会定期或在需要时将脏页通过FSD写回磁盘。
        * **内存“脏”页写 (System-wide Dirty Page Writing)**:
            * VMM也可能发起对系统中所有脏页 (不仅仅是文件缓存的) 的写回操作，以释放物理内存。
        * **创建节对象 (Section Object) for Memory Mapping**:
            * `NtCreateSection()` (内核API) -> `MmCreateSection()` (VMM函数)。VMM会与FSD交互来设置文件映射。

### 4.2 Windows 文件系统操作流程示例

1.  **(1) 显式文件I/O (Explicit File I/O - 如 `ReadFile`)**:
    * **应用程序调用**: `ReadFile` (Win32 API, 位于 Kernel32.dll 中)。
    * **转换到内核调用**: `ReadFile` 内部会调用 `NtReadFile` (Ntdll.dll, 内核模式转换层)。
    * **内核处理 (I/O Manager)**:
        1.  `NtReadFile` 进入内核模式。I/O管理器接收请求。
        2.  **句柄转换**: 将已打开文件的句柄 (Handle) 转换成对应的文件对象指针 (File Object Pointer)。
        3.  **权限检查**: 检查调用者是否有权限对该文件对象执行读操作。
        4.  **创建IRP**: I/O管理器创建一个IRP_MJ_READ类型的IRP (I/O Request Packet)。
        5.  **调用驱动程序**: 通过 `IoCallDriver` 将IRP传递给文件对象所在设备栈顶的驱动程序 (通常是文件系统驱动程序FSD，或其上的过滤驱动) 。
    * **FSD处理 (以缓存I/O为例)**:
        1.  FSD接收到IRP。如果该文件支持缓存 (通常是默认情况)。
        2.  FSD会调用 **Cache Manager** 的函数，例如 `CcCopyRead`。
        3.  **Cache Manager 检查私有缓存映射 (`PrivateCacheMap`)**:
            * **有效**: 表示该文件已经有相应的缓存结构初始化，可以直接尝试从缓存读取。
            * **无效**: 表示这是首次缓存访问或之前的映射已失效，需要调用 `CcInitializeCacheMap` 来初始化文件的缓存映射结构。
        4.  **从缓存读取**: `CcCopyRead` 尝试从系统文件缓存中读取数据。
            * **缓存命中**: 数据直接从缓存复制到用户缓冲区。
            * **缓存未命中**: `CcCopyRead` 会触发一个**缺页异常 (Page Fault)** (或者内部直接向FSD发起读请求)。这个缺页异常最终会导致VMM调用FSD从磁盘读取数据到缓存中，然后再复制到用户缓冲区。VMM中的处理函数可能是 `MmAccessFault` 或类似的缺页处理逻辑，它会构建新的IRP发送给FSD来从磁盘加载数据。

2.  **(2) 高速缓存延迟写、提前读 (Cache Lazy Write, Prefetch)**:
    * **高速缓存管理器的滞后写线程 (Lazy Writer Thread)**:
        * 定期地 (如每秒一次或系统空闲时) 对高速缓存中已被修改的页面 (脏页) 进行写操作。
        * 它通过调用内存管理器 (VMM) 的 `MmFlushSection` 函数来完成对特定文件节 (section) 的刷新。
        * 具体地说，`MmFlushSection` 最终可能会通过 `IoAsynchronousPageWrite` 构建IRP，将数据送交给FSD写回磁盘。
    * **高速缓存管理器的提前读线程 (Read-Ahead Thread)**:
        * 负责根据应用程序对文件的访问模式 (主要是顺序访问) 来提前读取数据到缓存中。
        * 它会分析已作的读操作，来决定提前读多少数据。
        * 提前读操作通常也是通过构建IRP并由缺页异常机制或专门的预读路径来完成的。

3.  **(3) 内存“脏”页写 (Memory "Dirty" Page Write - VMM initiated)**:
    * 除了Cache Manager的Lazy Writer，VMM本身也有**内存脏页写线程 (Modified Page Writer)**，它会定期地扫描物理内存，将修改过的页面 (不仅仅是文件缓存的脏页，也包括进程私有数据的脏页如果它们是页文件支持的) 写回到后备存储 (磁盘上的文件或页文件) 。
    * 该线程通过 `IoAsynchronousPageWrite` 来创建IRP写请求。这些IRP通常被标识为非缓存I/O (如果直接写页文件) 或通过文件系统路径 (如果写回文件) ，最终由FSD或磁盘驱动程序处理，将内存数据写到磁盘。

4.  **(4) 内存缺页处理 (Memory Page Fault Handling - VMM initiated)**:
    * **触发场景**:
        * 在进行显式I/O操作 (如`ReadFile`读入缓存未命中的数据) 时。
        * 高速缓存管理器进行提前读操作时。
        * 应用程序访问**内存映射文件 (Memory-Mapped File)** 且所需页面不在物理内存时。
        * 执行程序代码，但代码页不在内存时。
    * **处理流程**:
        * 发生缺页，CPU产生缺页中断。
        * **内存缺页处理函数 `MmAccessFault` (或类似函数) 在VMM中被调用。**
        * VMM判断缺页原因。如果页面数据应从文件加载，VMM会通过 `IoPageRead` (或类似的内部函数) 向文件所在的文件系统驱动程序 (FSD) 发送一个IRP_MJ_READ类型的IRP请求包，请求从磁盘读取相应的页面内容到物理内存中。

---
## 5. 分布式文件系统 (Distributed File System - DFS)

### 5.1 分布式文件系统 (1) - 概念

* **分布式计算机系统 (Distributed Computer System)**:
    * **定义**: 由多台地理位置分散的、通过网络互连的计算机组成的系统。这些计算机在逻辑上表现为一个统一的整体，但物理上是独立的。
    * **特点**: 强调**资源、任务、功能和控制的全面分布**。
    * **协作与自治**: 系统中的各个资源单元 (可以是物理的如服务器，或逻辑的如服务) 既相互协作以完成共同目标，又在一定程度上保持高度自治。
    * **目标**: 能够在全系统范围内实现资源共享和管理，动态进行任务分配或功能分配，支持并行运行分布式程序。
* **工作方式**:
    * **任务分布 (Task Distribution)**: 一个大的计算任务被分解成多个子任务，分配给系统中的不同计算机并行处理。
    * **功能分布 (Function Distribution)**: 系统的不同功能模块部署在不同的计算机上，各司其职，协同工作 (例如，Web服务器、应用服务器、数据库服务器分离) 。
* **分布式文件系统 (DFS)**:
    * **功能**: 完成的功能类似于传统操作系统中的本地文件系统——提供**永久性存储**和**共享文件**。
    * **核心特性**: 允许用户 (或应用程序) **直接存取远程服务器上的文件**，而不需要 (或感觉不到需要) 将它们显式地复制到本地计算机。用户体验上尽可能接近访问本地文件。

### 5.2 分布式文件系统 (2) - 透明性

* **系统的透明性 (Transparency)**: 指系统的内部实现细节对用户是隐藏的。用户感知不到或无需关心这些细节。DFS追求多种透明性：
    * **存取透明性 (Access Transparency)**:
        * **解释**: 用户以相同的方式访问本地文件和远程文件。使用相同的API或命令，无需区分文件是本地的还是远程的。
    * **位置透明性 (Location Transparency)**:
        * **解释**: 文件名不包含文件的物理位置信息 (如服务器名或路径) 。用户无需知道文件实际存储在哪台服务器上。即使文件从一个服务器迁移到另一个服务器，其命名和访问方式也不应改变。
    * **并发存取透明性 (Concurrency Transparency)**:
        * **解释**: 多个用户 (或进程) 可以并发地访问和操作共享文件，系统应能正确处理并发冲突，保证数据的一致性，且这种并发控制对用户是透明的。
    * **故障透明性 (Fault Transparency)**:
        * **解释**: 当系统中部分组件 (如某个服务器或网络连接) 发生故障时，系统应能尽可能地继续提供服务，或者优雅地处理故障，使得用户不受影响或受影响最小。例如，通过数据复制和故障切换。
    * **性能透明性 (Performance Transparency)**:
        * **解释**: 理想情况下，访问远程文件的性能应与访问本地文件相当，或者至少不应有数量级的下降。实际中难以完全达到，但系统会尽力优化。
    * **复制透明性 (Replication Transparency)**:
        * **解释**: 如果文件为了提高可用性或性能而在多个位置存放了副本，用户不应感知到这些副本的存在。系统自动管理副本的一致性和选择合适的副本进行访问。
    * **迁移透明性 (Migration Transparency)**:
        * **解释**: 文件或服务可以在系统中的不同节点之间迁移，而不会中断用户访问或要求用户更改访问方式。

### 5.3 Hadoop Distributed File System (HDFS) 实现机制概述

* **什么是HDFS**:
    * 一个设计用于在**商用硬件 (Commodity Hardware / Low-cost Hardware)** 集群上运行的**分布式文件系统**。
    * **核心特性**:
        * **高容错性 (Fault-tolerant)**: 能够自动处理硬件故障。
        * **高吞吐量 (High Throughput)**: 为批量数据处理 (如MapReduce) 优化，适合高数据访问速率。
        * **适用于超大数据集 (Large Data Set)**: 能够存储和管理PB级别的数据。
    * 通常作为 Apache Hadoop 生态系统的核心存储层。

* **HDFS 实现上的特点**:

    * **块存储 (Block Storage)**:
        * **什么是块存储**:
            * HDFS中最基本的存储单元是**数据块 (Block)**。
            * 数据块的大小是固定的，且相对较大，默认通常是 **64MB 或 128MB** (Hadoop 2.x 及以后版本默认为128MB)。
            * 如果一个文件的大小大于配置的数据块容量，文件将会被**拆分 (Split)** 成多个数据块进行存储。
            * 如果一个文件的大小小于数据块容量，它**不独占**整个数据块的物理存储空间 (即一个物理磁盘块上可以存储多个小文件的HDFS块，但HDFS逻辑上它还是一个块，只是实际占不满)。但HDFS中一个文件至少会占用一个块的元数据。
        * **为什么引入大块存储**:
            * **减少寻址开销**: 数据块比传统文件系统的磁盘块 (如4KB, 8KB) 大得多，这意味着一个大文件由更少的块组成。NameNode (元数据节点) 需要管理的块数量就更少，元数据规模相对较小，寻址效率高。
            * **支持大文件**: 文件以块为单位存储在集群的不同磁盘上，使得文件大小可以远超单个磁盘的容量。
            * **便于数据备份与并行处理**: 以数据块为单位进行复制和管理更简单。MapReduce等计算框架也以块为单位进行任务划分和并行处理。

    * **读写策略 (Read/Write Strategy)**:
        * **流式读写 (Streaming Data Access)**:
            * HDFS为流式数据访问进行了优化，即“一次写入，多次读取” (Write-Once-Read-Many, WORM) 的访问模式。
            * 数据写入后，通常不被修改，而是被大量地读取和分析。
        * **校验和 (Checksum) 校验**:
            * 在写入数据时，客户端会为每个数据块计算校验和，并将校验和与数据一同存储 (或由DataNode存储) 。读取数据时，会重新计算校验和并与存储的校验和进行比较，以检测数据损坏。
        * **Append 操作**: HDFS支持在现有文件的末尾追加数据 (Append)。
        * **不支持任意位置修改**: 不支持在文件的任意位置进行修改操作。如果需要修改，通常是重新写入整个文件或追加新版本。
        * **并发读文件**: 支持多个客户端并发读取同一个文件。
        * **并发写文件**: HDFS中一个文件在同一时间只能有一个写入者 (writer) 。
        * **为什么引入流式读写**:
            * **匹配应用场景**: 非常适合大数据分析场景，如日志分析、数据挖掘，这些场景通常是写入一次，然后反复读取进行分析。
            * **高吞吐量**: 优化了顺序读写，而不是低延迟的随机访问。

    * **数据冗余 (Data Redundancy)**:
        * **数据备份 (Replication)**:
            * HDFS通过在集群中的不同DataNode上存储每个数据块的多个副本 (replica) 来实现容错。
            * **复制因子 (Replication Factor)**: 每个块的副本数量，默认为 **3**。
        * **副本放置策略 (Replica Placement Policy) / 负载均衡**: 为了提高容错性和读取效率，副本的放置遵循一定策略：
            * **第一个副本**: 通常放在写入客户端所在的**本地节点 (Local Node)** (如果客户端本身是DataNode的话)，或者随机选择一个节点。
            * **第二个副本**: 放在与第一个副本**不同机架 (Rack)** 的某个节点上。
            * **第三个副本**: 放在与第二个副本**相同机架**但**不同节点**上。
            * **后续副本**: 随机放置在其他机架的节点上，但要避免一个机架有过多彩带副本。
            * **目的**: 即使整个机架发生故障 (如交换机故障、断电) ，数据仍然有副本可用。同时，同一机架内的副本可以利用机架内的高带宽进行读取或修复。
        * **机架感知 (Rack Awareness)**:
            * HDFS需要知道集群中DataNode的拓扑结构 (即哪些节点在哪个机架上) 。
            * 管理员通常需要配置一个脚本或程序，根据节点的IP地址或其他标识，将其映射到相应的机架位置。
            * NameNode利用机架信息来智能地放置副本和调度读请求。

    * **单点故障处理 (Single Point of Failure - SPOF Handling)**:
        * **HDFS 体系结构与单点故障点**:
            * HDFS采用**主从模式 (Master/Slave Architecture)**。
            * **名字节点 (NameNode - Master)**:
                * 是HDFS的**核心元数据服务器**。
                * **唯一存放文件系统元数据 (Metadata)** 的节点，包括文件系统的命名空间 (目录树) 、文件到数据块的映射、数据块的位置信息 (即哪些DataNode上有哪些块的副本) 等。
                * 执行对文件系统元数据的操作，如打开、关闭、重命名文件/目录。
                * 负责维护数据块到数据节点的映射，并指导客户端的读写操作。
                * 在早期HDFS版本中，NameNode是一个**单点故障点 (SPOF)**。如果NameNode宕机，整个HDFS集群将无法访问。
            * **数据节点 (DataNode - Slave)**:
                * 负责实际存储数据块。
                * 执行来自NameNode或客户端的读写块的请求。
                * 定期向NameNode发送心跳 (Heartbeat) 和块报告 (Block Report)，报告自身状态和所存储的块信息。
        * **高可用模式 (High Availability - HA)**: 为了解决NameNode的单点故障问题，Hadoop 2.x 引入了 NameNode HA。
            * **引入热备节点 (Standby NameNode)**:
                * 集群中运行两个NameNode：一个处于**Active**状态，另一个处于**Standby**状态。
                * Active NameNode负责所有客户端操作。Standby NameNode作为热备份，与Active NameNode保持元数据同步。
                * 当Active NameNode发生故障时，Standby NameNode可以快速切换为Active状态，接管服务，从而实现故障转移。
            * **元数据同步机制**: Active NameNode将其对元数据的修改 (编辑日志 edits) 写入到一个共享存储系统 (如NFS、JournalNode集群QJM) 。Standby NameNode从共享存储中读取这些编辑日志，并应用到自己的内存中，以保持与Active NameNode的元数据一致。
        * **数据持久化 (Checkpointing) 机制 (NameNode元数据)**:
            * NameNode的元数据主要存储在内存中以提高访问速度，但同时也需要持久化到磁盘以防丢失。
            * **编辑日志 (Edit Log)**: 记录了自上次快照以来所有对文件系统元数据产生改变的操作 (事务日志) 。这是一个只追加写的文件。
            * **镜像文件 (FsImage - Filesystem Image)**: 是某一时刻文件系统元数据在内存中的完整快照，是元数据在磁盘上的一个副本。
            * **Checkpointing过程**: 定期地 (或当Edit Log达到一定大小时) ，系统会将当前的FsImage与Edit Log合并，生成一个新的、更新的FsImage，并清空旧的Edit Log (或开始新的Edit Log) 。这个过程称为**检查点 (Checkpoint)**。
            * **作用**: 加快NameNode启动速度 (只需加载最新的FsImage和之后的一小部分Edit Log，而无需回放所有历史操作) 。

    * **HDFS HA 中需要备份和同步的数据**:
        * **事务日志 (Edit Log)**: 必须实时或准实时地同步到共享存储，并被Standby NameNode读取。
        * **镜像文件 (FsImage)**: Standby NameNode也需要定期获取或生成FsImage的副本。
        * **数据块到数据节点的映射关系**: 虽然主要由Active NameNode管理，但DataNode会向两个NameNode都汇报块信息，以便Standby NameNode在切换后能快速知道块的位置。
        * **数据热备份 (Standby NameNode 的作用)**:
            * Standby NameNode通过持续读取共享存储中的Edit Log并应用到自己的FsImage副本，来实时或近实时地维护一个与Active NameNode一致的元数据镜像。

    * **HDFS 其他特性**:
        * **安全模式 (Safe Mode)**: NameNode启动时会进入安全模式。在此模式下，HDFS只接受读请求，不接受写请求或元数据修改。NameNode会等待足够多的DataNode汇报它们的块信息，直到系统达到一定的数据块完整性百分比后，才会退出安全模式。
        * **缓存一致性控制 (Cache Coherency)**: HDFS的缓存模型相对简单，因为它主要是为WORM场景设计的。客户端通常直接与DataNode交互获取数据。
        * **文件删除后恢复 (Trash Feature)**: 删除的文件可以被移到一个特殊的".Trash"目录，在配置的时间内可以恢复，防止误删除。
        * **Kerberos 安全机制**: HDFS支持使用Kerberos进行用户认证和授权，以增强安全性。

### 5.4 拓展：幻方 3FS (Fire-Flyer File System by Quantitative Investment Firm "幻方量化")

* **名称**: 幻方3FS (Fire-Flyer File System)
* **定位**: 一个为AI训练 (特别是量化投资领域可能涉及的大量小文件、随机读场景) 优化的分布式文件系统。
* **架构组件**:
    * **集群管理器 (Cluster Manager)**
    * **元数据服务 (Metadata Service)**
    * **存储服务 (Storage Service)**
    * **客户端 (Client)**
* **针对训练场景的优化**:
    * AI训练 (尤其是某些类型) 可能涉及对大量小文件进行**大量随机读取**，导致**局部性差**，传统文件系统的**文件缓存 (File Cache)** 效果不佳。
    * **客户端元数据缓存**: 文件元数据 (如文件位置信息) 可以在客户端进行缓存。后续对同一文件的访问可以直接利用缓存的元数据信息，**跳过对元数据服务器的请求**，从而**降低访问延迟**。
    * **异步零拷贝设计**: 基于 Linux 的 `io_uring` 异步I/O接口，旨在减少数据在内核和用户空间之间的拷贝次数，提高I/O效率。
    * **存储策略**:
        * **条带化存储 (Striping)**: 将数据分片存储在多个存储节点上，提高并行度。
        * **链式复制 (Chained Replication)**: 数据副本的写入沿着一个链条顺序进行，可能用于保证写入顺序或简化一致性。
        * **CRAQ (Chain Replication with Apportioned Queries)**: 一种复制协议，通常是写操作到链头，读操作可以到链上任一可用副本 (或特定副本以分摊负载) 。
        * **目标**: 实现**高吞吐**和**强一致性**。

---

## 6. 重点小结

* **文件系统的管理**:
    * **文件系统的备份**: 全量备份、增量备份，用于数据恢复和容灾。
    * **文件系统的一致性**: `fsck`工具，写操作策略 (通写、延迟写、日志/可恢复写) 保证元数据和数据的正确性。
* **文件系统的性能优化**:
    * **块高速缓存 (Block Cache)**: 利用内存缓存磁盘块，减少磁盘I/O。
    * **磁盘调度 (Disk Scheduling)**: FCFS, SSTF, SCAN, C-SCAN等算法优化磁头寻道。
    * **RAID技术 (RAID)**: 通过条带化和冗余提高性能和可靠性。
    * 其他：提前读取、合理分配磁盘空间、信息优化分布、记录成组等。
* **文件系统的结构设计**:
    * **文件系统的层次模型**: 将文件系统功能划分为不同层次，如逻辑层、组织层、基本I/O层。
    * **虚拟文件系统 (VFS)**: 提供统一接口，支持多种底层具体文件系统。
    * **日志结构文件系统 (LFS)**: 以日志方式追加写入所有数据和元数据，提高写性能，依赖清理过程。
    * **日志文件系统 (Journaling FS)**: 通过记录元数据变更日志，实现快速崩溃恢复和一致性。
